进程调度机制
linux4.9

kernel/sched/core.c
===================
Kernel scheduler and related syscalls



structure
---------
1.就绪队列
内核为每一个cpu创建一个进程就绪队列，该队列上的进程均由该cpu执行.

DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues); //[kernel/sched/core.c]
定义了一个 struct rq 结构体数组，每个数组元素是一个就绪队列，对应一个cpu。

[kernel/sched/sched.h]
// This is the main, per-CPU runqueue data structure.
struct rq {
	...

	struct cfs_rq cfs; //CFS-related fields in a runqueue
	struct rt_rq rt; //Real-Time classes' related field in a runqueue:
	struct dl_rq dl; //Deadline class' related fields in a runqueue
	...

	/* cpu of this runqueue: */
	int cpu;

	...
};

struct rq struct 是本地cpu所有进程组成的就绪队列，内嵌了 3 个子就绪队列:
	<1> struct cfs_rq cfs  组织普通进程, 采用完全公平调度策略cfs
	<2> struct rt_rq rt    组织 实时进程, 采用实时调度策略
	<3> struct dl_rq dl    调度空闲进程。

研究普通进程的调度: 关心 struct cfs_rq cfs 队列；
研究实时进程: 关心 struct rt_rq rt队列。

1.1普通进程的就绪队列struct cfs_rq [kernel/sched/sched.h]
---------------------------------------------------------

/* CFS-related fields in a runqueue */
struct cfs_rq {
	...
	struct rb_root tasks_timeline;
	struct rb_node *rb_leftmost;

	struct sched_entity *curr, *next, *last, *skip;

	...
	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
	...
};

cfs_rq 就绪队列是以红黑树的形式来组织调度实体。
tasks_timeline 就是红黑树的树根。
rb_leftmost 指向了红黑树最左边的左孩子（下一个可调度的实体）。
curr 指向当前正运行的实体，next 指向将被唤醒的进程，last 指向唤醒next进程的进程。
rq 指向了该cfs_rq就绪队列所属的rq队列。


1.2实时进程的就绪队列struct rt_rq [kernel/sched/sched.h]
-------------------------------------------------------

/* Real-Time classes' related field in a runqueue: */
struct rt_rq {
	...

};


2.调度实体
----------
[include/linux/sched.h]

2.1 普通进程的调度实体sched_entity

struct sched_entity {
	...
	struct rb_node		run_node;
	...
#ifdef CONFIG_FAIR_GROUP_SCHED
	int			depth;
	struct sched_entity	*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq		*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq		*my_q;
#endif
	...
};

每个进程描述符中均包含一个struct sched_entity 变量，内核使用该结构体来将普通进程
组织到采用完全公平调度策略的就绪队列中（struct rq中的cfs队列中），

struct sched_entity 有两个作用:
	一是, 包含有进程调度的信息
	（比如进程的运行时间，睡眠时间等，调度程序参考这些信息决定是否调度进程），
	二是, 使用该结构体来组织进程，

	truct rb_node 类型结构体变量run_node是红黑树节点，struct sched_entity 调度实体将被
	组织成红黑树的形式，意味着普通进程也被组织成红黑树的形式。
	CONFIG_FAIR_GROUP_SCHED 是和组调度有关的成员，需要开启组调度。
	parent 顾名思义指向了当前实体的上一级实体。
	fs_rq 指向了该调度实体所在的就绪队列。

	my_q 指向了本实体拥有的就绪队列（调度组），该调度组（包括组员实体）属于下一个级别，
	和本实体不在同一个级别，该调度组中所有成员实体的parent域指向了本实体，这就解释了
	上边的parent.
	depth代表了此队列（调度组）的深度，每个调度组都比其parent调度组深度大1。
	内核依赖my_q域实现组调度。

2.2 实时进程的调度实体 sched_rt_entity

struct sched_rt_entity {
	struct list_head run_list;
	...

#ifdef CONFIG_RT_GROUP_SCHED
	struct sched_rt_entity	*parent;
	/* rq on which this entity is (to be) queued: */
	struct rt_rq		*rt_rq;
	/* rq "owned" by this entity/group: */
	struct rt_rq		*my_q;
#endif
};
struct sched_rt_entity 用来组织实时进程，实时进程被组织到 struct rq 中的rt队列中。
每个进程描述符中也包含一个struct sched_rt_entity结构体。
struct sched_rt_entity 结构体中并未包含struct rb_node类型结构体变量，而在第1行出现了
struct list_head类型结构体变量run_list，实时进程的就绪队列是双向链表形式，而不是红黑数的形式。

3.调度类
--------
[kernel/sched/sched.h]

struct sched_class {
	const struct sched_class *next;
	...
};

内核声明了一个调度类sched_class的结构体类型，用来实现不同的调度策略，
该结构体成员都是函数指针，这些指针指向的函数就是调度策略的具体实现，
所有和进程调度有关的函数都直接或者间接调用了这些成员函数，来实现进程调度。

每个进程描述符中都包含一个指向该结构体类型的指针sched_class，指向了所采用的调度类。

完全公平调度策略类的定义和初始化:
[kernel/sched/fair.c]
// Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)

定义一个全局的调度策略变量:
const struct sched_class fair_sched_class;

初始化:
// All the scheduling class methods:
const struct sched_class fair_sched_class = {
	.next			= &idle_sched_class,
	...
};



4.进程描述符task_struct
-----------------------
[include/linux/sched.h]

struct task_struct {
	...
	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
	...
	int on_rq;

	int prio, static_prio, normal_prio;
	unsigned int rt_priority;
	const struct sched_class *sched_class;
	struct sched_entity se;
	struct sched_rt_entity rt;
#ifdef CONFIG_CGROUP_SCHED
	struct task_group *sched_task_group;
#endif
	struct sched_dl_entity dl;
	...
	unsigned int policy;
	...
#ifdef CONFIG_SCHED_INFO
	struct sched_info sched_info;
#endif
	...
};

只列出了和进程调度有关的成员。
int prio, static_prio, normal_prio; 三个变量代表了普通进程的三个优先级，
rt_priority 代表了实时进程的优先级。
关于进程优先级的概念，可以看《深入理解linux内核架构》这本书的进程管理章节。

const struct sched_class *sched_class;
struct sched_entity se;
struct sched_rt_entity rt;
是我们上边提到的那些结构体在进程描述符中的定义。

policy代表了当前进程的调度策略，内核给出了宏定义，它可以在这些宏中取值，
关于详细的讲解还是去看《深入理解linux内核架构》这本书的进程管理部分。
policy取了什么值，sched_class也应该取相应的调度类指针。



进程调度过程分析：
=================

进程调度过程分为两部分:
	一是对进程信息进行修改，主要是修改和调度相关的信息，比如进程的运行时间，睡眠时间，进程的状态，cpu的负荷等等，
	二是进程的切换。
	和进程调度相关的所有函数中，只有schedule函数是用来进行进程切换的，其他函数都是用来修改
	进程的调度信息。
	schedule函数分析//todo。
	对于其他函数，我们将按照其功能，逐类来分析。

1.scheduler_tick[kernel/sched/core.c ]
----------------
// This function gets called by the timer code, with HZ frequency.
// We call it with interrupts disabled.

该函数被时钟中断处理程序调用，将当前cpu的负载情况记载到运行队列struct rq的
某些成员中，并更新当前进程的时间片。

void scheduler_tick(void)
{
	int cpu = smp_processor_id(); //取当前的cpu号
	struct rq *rq = cpu_rq(cpu); //获取当前cpu的就绪队列（每个cpu有一个）rq
	struct task_struct *curr = rq->curr; //从就绪队列中获取当前运行进程的描述符

	sched_clock_tick();

	raw_spin_lock(&rq->lock);
	//更新就绪队列中的clock和clock_task成员值，代表当前的时间，一般我们会用到clock_task。
	update_rq_clock(rq);
	//进入当前进程的调度类的task_tick函数中，更新当前进程的时间片，不同调度类该函数实现不同
	curr->sched_class->task_tick(rq, curr, 0);
	//更新就绪队列的cpu负载信息
	cpu_load_update_active(rq);
	calc_global_load_tick(rq);
	raw_spin_unlock(&rq->lock);

	perf_event_task_tick();

#ifdef CONFIG_SMP
	//判断当前cpu是否是空闲的，是的话idle_cpu返回1，否则返回0。
	rq->idle_balance = idle_cpu(cpu);
	//挂起SCHED_SOFTIRQ软中断函数，去做周期性的负载平衡操作。
	trigger_load_balance(rq);
#endif
	//将最新的时钟滴答数jiffies存入就绪队列的last_sched_tick域中
	rq_last_tick_reset(rq);
}

task_tick_fair函数[kernel/sched/fair.c]:
------------------
完全公平调度类的 task_tick

const struct sched_class fair_sched_class = {
	...
	.task_tick		= task_tick_fair,
	...
};

如果某个进程的调度类采用完全公平调度类，上个函数scheduler_tick 所执行的task_tick函数指针，
就指向了task_tick_fair函数。
完全公平调度对象的初始化，赋值语句.task_tick = task_tick_fair。


/*
 * scheduler tick hitting a task of our scheduling class:
 */
static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
{
	struct cfs_rq *cfs_rq;
	struct sched_entity *se = &curr->se; //获取当前进程的普通调度实体, 将指针存放到se中

	/*
	 * 遍历当前调度实体的上一级实体，以及上上一级实体，以此类推，
	 * 然后在entity_tick函数中更新调度实体的运行时间等信息。
	 * 循环遍历的原因:
	 * 当开启了组调度后，调度实体的parent域存储它的上一级节点，该
	 * 实体和它parent指向的实体不是同一级别，因此使用循环就把从当
	 * 前级别（组）到最顶层的级别遍历完；
	 * 如果未选择组支持，则循环只执行一次，仅对当前调度实体进行更新。
	 */
	for_each_sched_entity(se) { //Walk up scheduling entities hierarchy
		cfs_rq = cfs_rq_of(se); //runqueue on which this entity is (to be) queued
		entity_tick(cfs_rq, se, queued);
	}

	if (static_branch_unlikely(&sched_numa_balancing))
		task_tick_numa(rq, curr);
}



entity_tick[kernel/sched/fair.] :
-----------
在该函数中对调度实体（进程）的运行时间等信息进行更新。

static void
entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
{
	/*
	 * Update run-time statistics of the 'current'.
	 */
	update_curr(cfs_rq); //对当前进程的运行时间进行更新

	/*
	 * Ensure that runnable average is periodically updated.
	 */
	update_load_avg(curr, 1);
	update_cfs_shares(cfs_rq);

#ifdef CONFIG_SCHED_HRTICK
	/*
	 * queued ticks are scheduled to match the slice, so don't bother
	 * validating it and just reschedule.
	 */
	//如果传进来的参数queued不为0的话，当前进程会被无条件设置重新调度标志（允许被抢占了）。
	if (queued) {
		resched_curr(rq_of(cfs_rq));
		return;
	}
	/*
	 * don't let the period tick interfere with the hrtick preemption
	 */
	if (!sched_feat(DOUBLE_TICK) &&
			hrtimer_active(&rq_of(cfs_rq)->hrtick_timer))
		return;
#endif

	// 如果当前cfs_rq队列等待调度的进程数量大于1，执行check_preempt_tick
	// 函数检查当前进程的时间片是否用完，用完就需要调度别的进程来运行
	//（如果当前进程“真实时间片”用完，该函数给当前进程设置need_resched标志，
	// 然后schedule程序就可以重新在就绪队列中调度新的进程）
	if (cfs_rq->nr_running > 1)
		check_preempt_tick(cfs_rq, curr);
}

update_curr [kernel/sched/fair.c]:
-----------

更新进程运行时间最核心的一个函数。
/*
 * Update the current task's runtime statistics.
 */
static void update_curr(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq->curr; //获取当前的调度实体
	//从就绪队列rq的clock_task成员中获取当前时间，存入now中
	u64 now = rq_clock_task(rq_of(cfs_rq));
	u64 delta_exec;

	if (unlikely(!curr))
		return;

	//用当前时间减去进程在上次时钟中断tick中的开始时间得到进程运行的时间间隔，存入delta_exec中。
	delta_exec = now - curr->exec_start;
	if (unlikely((s64)delta_exec <= 0))
		return;

	//当前时间又成为进程新的开始时间。
	curr->exec_start = now;

	schedstat_set(curr->statistics.exec_max,
		      max(delta_exec, curr->statistics.exec_max));

	//将进程运行的时间间隔delta_exec累加到调度实体的sum_exec_runtime成员中，
	//该成员代表进程到目前为止运行了多长时间。
	curr->sum_exec_runtime += delta_exec;
	//将进程运行的时间间隔delta_exec也累加到公平调度就绪队列cfs_rq的exec_clock成员中。
	schedstat_add(cfs_rq->exec_clock, delta_exec);

	// 很关键，它将进程执行的真实运行时间转换成虚拟运行时间，然后累加到调度实体的
	// vruntime域中，进程的虚拟时间非常重要，完全公平调度策略就是依赖该时间进行调度。
	// 关于进程的真实时间和虚拟时间的概念，以及它们之间的转换算法，后面会提到，
	// 详细的内容大家可以看看《深入理解linux内核架构》的进程管理章节。
	curr->vruntime += calc_delta_fair(delta_exec, curr);
	// 更新cfs_rq队列中的最小虚拟运行时间min_vruntime，该时间是就绪队列中所有进程
	// 包括当前进程的已运行的最小虚拟时间，只能单调递增，
	update_min_vruntime(cfs_rq);

	//如果调度单位是进程（不是组），会更新进程描述符中的运行时间。
	if (entity_is_task(curr)) {
		struct task_struct *curtask = task_of(curr);

		trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
		cpuacct_charge(curtask, delta_exec);
		account_group_exec_runtime(curtask, delta_exec);
	}

	//更新cfs_rq队列的剩余运行时间，并计算出期望运行时间，必要的话可以对进程重新调度。
	account_cfs_rq_runtime(cfs_rq, delta_exec);
}

update_min_vruntime [kernel/sched/fair.c]:
-------------------


static void update_min_vruntime(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq->curr;

	u64 vruntime = cfs_rq->min_vruntime;

	//如果当前有进程正在执行，将当前进程已运行的虚拟时间保存在vruntime变量中。
	if (curr) {
		if (curr->on_rq)
			vruntime = curr->vruntime;
		else
			curr = NULL;
	}

	//如果就绪队列中有下一个要被调度的进程（由rb_leftmost指针指向），则进入if
	if (cfs_rq->rb_leftmost) {
		struct sched_entity *se = rb_entry(cfs_rq->rb_leftmost,
						   struct sched_entity,
						   run_node);

		//从当前进程和下个被调度进程中，选择最小的已运行虚拟时间，保存到vruntime中。
		//保证了队列的min_vruntime域中存放的是就绪队列中最小的虚拟运行时间
		if (!curr)
			vruntime = se->vruntime;
		else
			vruntime = min_vruntime(vruntime, se->vruntime);
	}

	/* ensure we never gain time by being placed backwards. */
	// 从当前队列的min_vruntime域和vruntime变量中，选最大的保存到队列的
	// min_vruntime域中，完成更新。
	//保证min_vruntime域中的值单调递增
	cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);
#ifndef CONFIG_64BIT
	smp_wmb();
	cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;
#endif
}

每个cfs_rq队列均有一个min_vruntime成员，装的是就绪队列中所有进程包括当前进程已运行的虚拟时间
中最小的那个时间。本函数来更新这个时间。

队列中的min_vruntime成员非常重要，用于在睡眠进程被唤醒后以及新进程被创建好时，
进行虚拟时间补偿或者惩罚。



calc_delta_fair [kernel/sched/fair.c]
---------------

/*
 * delta /= w
 */
static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
{
	// 判断当前进程nice值是否为0，如果是，直接返回真实运行时间delta
	//（nice0级别的进程真实运行时间和虚拟运行时间值相等）
	if (unlikely(se->load.weight != NICE_0_LOAD))
		delta = __calc_delta(delta, NICE_0_LOAD, &se->load); // 如果不是，将真实时间转换成虚拟时间。

	return delta;
}

__calc_delta函数[kernel/sched/fair.c]
------------

/*
 * delta_exec * weight / lw.weight
 *   OR
 * (delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT
 *
 * Either weight := NICE_0_LOAD and lw \e sched_prio_to_wmult[], in which case
 * we're guaranteed shift stays positive because inv_weight is guaranteed to
 * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift >= 22.
 *
 * Or, weight =< lw.weight (because lw.weight is the runqueue weight), thus
 * weight/lw.weight <= 1, and therefore our shift will also be positive.
 */
static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)
{
	u64 fact = scale_load_down(weight);
	int shift = WMULT_SHIFT;

	__update_inv_weight(lw);

	if (unlikely(fact >> 32)) {
		while (fact >> 32) {
			fact >>= 1;
			shift--;
		}
	}

	/* hint to use a 32x32->64 mul */
	fact = (u64)(u32)fact * lw->inv_weight;

	while (fact >> 32) {
		fact >>= 1;
		shift--;
	}

	return mul_u64_u32_shr(delta_exec, fact, shift);
}

该函数执行了两种算法：
	delta_exec * weight / lw.weight，
	OR
	(delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT

	计算的结果就是转换后的虚拟时间。

	至此，scheduler_tick函数大致就分析完了，当然还有一些细节没有分析到，进程调度这块非常庞杂，
	要想把所有函数分析完非常耗费时间和精力，以后如果分析到的话，再慢慢补充。
	scheduler_tick 函数主要更新进程的运行时间，包括物理时间和虚拟时间。

2.进程优先级设置的函数，
-----------------------
进程的优先级和调度关系密切，计算进程的虚拟运行时间要用到优先级，优先级决定进程
权重，权重决定进程虚拟时间的增加速度，最终决定进程可运行时间的长短。权重越大的
进程可以执行的时间越长。

从effective_prio函数开始[kernel/sched/core.c]:
------------------------

/*
 * Calculate the current priority, i.e. the priority
 * taken into account by the scheduler. This value might
 * be boosted by RT tasks, or might be boosted by
 * interactivity modifiers. Will be RT if the task got
 * RT-boosted. If not then it returns p->normal_prio.
 */
static int effective_prio(struct task_struct *p)
{
	//设置进程的普通优先级
	p->normal_prio = normal_prio(p);
	/*
	 * If we are RT tasks or we were boosted to RT priority,
	 * keep the priority unchanged. Otherwise, update priority
	 * to the normal priority:
	 */
	// 通过进程的活动优先级可以判断出该进程是不是实时进程，如果是实时进程，
	// 则返回p->prio，实时进程的活动优先级不变。否则返回p->normal_prio，这
	// 说明普通进程的活动优先级等于它的普通优先级。
	if (!rt_prio(p->prio))
		return p->normal_prio;
	return p->prio;
}

effective_prio 在进程创建时或者在用户的nice系统调用中都会被调用到，来设置进程
的活动优先级（进程的三种优先级：活动优先级prio，静态优先级static_prio，普通优先级normal_prio），该函数设计的有一定技巧性，函数的返回值是用来设置进程的活动优先
级，但是在函数体中也把进程的普通优先级设置了。

normal_prio函数[kernel/sched/core.c]
---------------
/*
 * Calculate the expected normal priority: i.e. priority
 * without taking RT-inheritance into account. Might be
 * boosted by interactivity modifiers. Changes upon fork,
 * setprio syscalls, and whenever the interactivity
 * estimator recalculates.
 */
static inline int normal_prio(struct task_struct *p)
{
	int prio;

	//判断当前进程是不是空闲进程，是的话设置进程的普通优先级为-1
	//（-1是空闲进程的优先级）
	if (task_has_dl_policy(p))
		prio = MAX_DL_PRIO-1;
	// 否则, 判断进程是不是实时进程，是的话设置实时进程的普通优先级为0-99（数字
	// 越小优先级越高），这块减去了p->rt_priority，比较奇怪，这是因为实时进程
	// 描述符的rt_priority成员中事先存放了它自己的优先级（数字也是0-99，但在这
	// 里数字越大，优先级越高），因此往p->prio中倒换的时候，需要处理一下，
	// MAX_RT_PRIO值为100，因此MAX_RT_PRIO-1-（0，99）就倒换成了（99，0），
	// 这仅仅是个小技巧。
	else if (task_has_rt_policy(p))
		prio = MAX_RT_PRIO-1 - p->rt_priority;
	// 如果当前进程也不是实时进程（是普通进程喽），则将进程的静态优先级存入prio
	// 中，然后返回（也就是说普通进程的普通优先级等于其静态优先级）。
	else
		prio = __normal_prio(p);
	return prio;
}

normal_prio用来设置进程的普通优先级。


总结:
	共有三种进程：空闲进程，实时进程，普通进程；
	每种进程都包含三种优先级：活动优先级，普通优先级，静态优先级。

	空闲进程的普通优先级: 永远-1，
	实时进程的普通优先级: 根据p->rt_priority设定（0-99），
	普通进程的普通优先级: 根据其静态优先级设定（100-139）。


3.进程权重设置的函数，
--------------------
 +--------+   决定    +------+  决定      +--------------+
 | 优先级 |---------->| 权重 |----------->| 运行时间长短 |
 +--------+           +------+            +--------------+




权重相关的数据结构
------------------

[include/linux/sched.h]
struct load_weight {
	unsigned long weight;
	u32 inv_weight;
};

每个 进程描述符，调度实体，就绪对列 中都包含一个该类型变量，用来保存各自的权重。

成员weight中存放进程优先级所对应的权重。
成员inv_weight中存放NICE_0_LOAD/weight的结果，这个结果乘以进程运行的时间间隔

delta_exec就是进程运行的虚拟时间。
因此引入权重就是为了计算进程的虚拟时间。
在这里将中间的计算结果保存下来，后边就不用再计算了，直接可以用。

[kernel/sched/sched.h]
数组sched_prio_to_weight[40]
---------------------

/*
 * Nice levels are multiplicative, with a gentle 10% change for every
 * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
 * nice 1, it will get ~10% less CPU time than another CPU-bound task
 * that remained on nice 0.
 *
 * The "10% effect" is relative and cumulative: from _any_ nice level,
 * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
 * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
 * If a task goes up by ~10% and another task goes down by ~10% then
 * the relative distance between them is ~25%.)
 */
const int sched_prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};

该数组是普通进程的优先级和权重对应关系。
数组元素是权重值，分别是优先级从100-139或者nice值从-20-+19所对应的权重值。
nice值(-20-+19)是从用户空间看到的普通进程的优先级，和内核空间的优先级(100-139) 一一对应。
struct load_weight中的weight成员存放正是这些权重值。 

中间结果数组 sched_prio_to_wmult[40] （kernel/sched/sched.h）
------------
/*
 * Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated.
 *
 * In cases where the weight does not change often, we can use the
 * precalculated inverse to speed up arithmetics by turning divisions
 * into multiplications:
 */
const u32 sched_prio_to_wmult[40] = {
 /* -20 */     48388,     59856,     76040,     92818,    118348,
 /* -15 */    147320,    184698,    229616,    287308,    360437,
 /* -10 */    449829,    563644,    704093,    875809,   1099582,
 /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
 /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
 /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
 /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
 /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
};

sched_prio_to_wmult 数组元素就是上个数组元素被NICE_0_LOAD除的结果，一一对应。
struct load_weight中的inv_weight成员存放正是这些值。

和权重设置相关的函数。
--------------------
从set_load_weight函数开始 [kernel/sched/core.c]:

static void set_load_weight(struct task_struct *p)
{
	// 进程p的静态优先级转换成数组下标（减去100，从100-139--->0-39）
	int prio = p->static_prio - MAX_RT_PRIO;
	//获取当前进程的调度实体中的权重结构体指针，存入load中
	struct load_weight *load = &p->se.load;

	/*
	 * SCHED_IDLE tasks get minimal weight:
	 */
	//当前进程是空闲进程，则设置相应的权重和中间计算结果
	if (idle_policy(p->policy)) {
		load->weight = scale_load(WEIGHT_IDLEPRIO);
		load->inv_weight = WMULT_IDLEPRIO;
		return;
	}

	// 设置实时进程和普通进程的权重和中间计算结果。
	load->weight = scale_load(sched_prio_to_weight[prio]);
	load->inv_weight = sched_prio_to_wmult[prio];
}

 set_load_weight用来设置进程p的权重。


就绪队列中也包含权重结构体，使用update_load_set 设置。[kernel/sched/fair.c]
--------------
static inline void update_load_set(struct load_weight *lw, unsigned long w)
{
	lw->weight = w;
	lw->inv_weight = 0;
}
update_load_set 设置就绪队列的权重



static inline void update_load_add(struct load_weight *lw, unsigned long inc)
{
	lw->weight += inc;
	lw->inv_weight = 0;
}
当有进程加入就绪队列，使用update_load_add增加就绪队列的权重。


static inline void update_load_sub(struct load_weight *lw, unsigned long dec)
{
	lw->weight -= dec;
	lw->inv_weight = 0;
}
当有进程从就绪队列移除时，使用该函数减少就绪队列的权重。

就绪队列的load_weight.inv_weight成员总是0，因为不会使用到就绪队列的该域。


-----------------
//todo
 struct task ->

